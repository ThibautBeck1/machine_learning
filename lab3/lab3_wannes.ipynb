{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Lab 3\n",
    "## Classification\n",
    "TA: Bryan Coulier (bryan.coulier@kuleuven.be)\n",
    "\n",
    "### Simple Classification\n",
    "Create and train the following classification models for the iris dataset:\n",
    "- K-nearist neighbor (n=5)\n",
    "- Support-Vector machines\n",
    "- Gaussian Naive Bayes\n",
    "- Decision Tree Classifier\n",
    "\n",
    "Determine the accuracy of each trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train) # just remember the training data\n",
    "\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"KNN Accuracy:\", accuracy_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC() # works with a hyperplane, if needed, \n",
    "# it will use a kernel trick: mapping the data to a higher dimension\n",
    "# a kernel is a function that computes the inner product of two data points in a high-dimensional feature space\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train) # assuming that the features are independent\n",
    "# it also assumes that the features are normally distributed\n",
    "# formula: P(y|x) = P(x|y)*P(y) / P(x) where y is the class and x is the feature\n",
    "\n",
    "y_pred_gnb = gnb.predict(X_test) # calculate the mean and variance of each feature for each class, also the prior probability of each class\n",
    "\n",
    "accuracy_gnb = accuracy_score(y_test, y_pred_gnb)\n",
    "print(\"Gaussian Naive Bayes Accuracy:\", accuracy_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42) # random_state is used to reproduce the same results\n",
    "dt.fit(X_train, y_train) # starts with the root node, and then splits the data into subsets\n",
    "# decision nodes are used to split the data, and leaf nodes are used to predict the outcome\n",
    "# gini impurity is used to decide the best split (how well the data is split)\n",
    "# or entropy is used to decide the best split (how much information is gained)\n",
    "\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models achieve 100% accuracy on the test set, which can be expected because the Iris dataset is relatively simple and well-separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "Determine the mean accuracy and standard deviation with a confidence interval of 98% for the KNN-model by using 5-fold cross-validation.\n",
    "\n",
    "Explain why you chose a certain method to determine the confidence interval and how it is calculated.\n",
    "\n",
    "What does this confidence interval tell you about the accuracy of the model?\n",
    "\n",
    "In case you assume a normal distribution, justify this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.96666667 1.         0.93333333 0.96666667 1.        ]\n",
      "Mean Accuracy: 0.9733333333333334\n",
      "Standard Deviation: 0.02494438257849294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Create the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
    "# 5-fold cross-validation means that the data is split into 5 parts, and the model is trained on 4 parts and tested on 1 part\n",
    "# this is done 5 times, and the accuracy is calculated each time\n",
    "\n",
    "# Calculate mean accuracy and standard deviation\n",
    "mean_accuracy = np.mean(cv_scores)\n",
    "std_accuracy = np.std(cv_scores)\n",
    "\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n",
    "print(\"Standard Deviation:\", std_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "98% Confidence Interval: (np.float64(0.9315343853193323), np.float64(1.0151322813473345))\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# Number of samples (folds)\n",
    "n = len(cv_scores)\n",
    "\n",
    "# Degrees of freedom\n",
    "dof = n - 1\n",
    "\n",
    "# Calculate the t-value for a 98% confidence interval\n",
    "t_value = t.ppf(0.99, dof)  # 0.99 because it's two-tailed and we want 98% confidence\n",
    "# ppf is the percent point function, it is used to calculate the t-value for a given probability and degrees of freedom\n",
    "\n",
    "# Calculate the margin of error\n",
    "margin_of_error = t_value * (std_accuracy / np.sqrt(n))\n",
    "# the margin of error is used to calculate the confidence interval\n",
    "\n",
    "# Calculate the confidence interval\n",
    "confidence_interval = (mean_accuracy - margin_of_error, mean_accuracy + margin_of_error)\n",
    "\n",
    "print(\"98% Confidence Interval:\", confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross validation**\n",
    "\n",
    "Cross-validation is a technique to assess how well a machine learning model will perform on unseen data. It’s like a practice run to avoid overfitting.\n",
    "\n",
    "- Split your dataset into k equal parts (folds). Here, k=5, so 5 folds.\n",
    "- Train the model on k-1 folds (4 folds) and test it on the remaining 1 fold.\n",
    "- Repeat this k times, each time using a different fold as the test set.\n",
    "- Calculate a performance metric (e.g., accuracy) for each fold.\n",
    "- Average these metrics to estimate the model’s overall performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for determining the confidence interval: **t-distribution**\n",
    "- The sample size is small (n=5 folds in cross-validation), so the t-distribution is more appropriate than the normal distribution.\n",
    "- The population standard deviation is unknown (we only have the sample standard deviation).\n",
    "- The t-distribution is specifically designed for small sample sizes and accounts for the additional uncertainty introduced by estimating the standard deviation from the sample.\n",
    "\n",
    "What does this confidence interval tell you about the accuracy of the model?\n",
    "- The 98% confidence interval for the mean accuracy is approximately (0.946,1.014).\n",
    "- This means we are 98% confident that the true mean accuracy of the KNN model lies within this range.\n",
    "- The interval is very close to 1, indicating that the model performs exceptionally well on the Iris dataset.\n",
    "\n",
    "Assumption of Normality:\n",
    "- The t-distribution is used because of the small sample size and unknown population standard deviation.\n",
    "- The assumption of normality is reasonable due to the robustness of the t-distribution and the symmetry of the cross-validation scores.\n",
    "- If we wanted a more distribution-free approach, we could use bootstrapping to construct a confidence interval without assuming normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter Tuning\n",
    "Use Grid search to tune the hyperparameters for Gaussian Naive Bayes and retrain the model with these hyper-parameters. \n",
    "\n",
    "Use 5-fold Cross Validation for the grid search.\n",
    "\n",
    "Print out the selected Hyperparameter values and their grid search score.\n",
    "\n",
    "Determine the accuracy of the retrained model, and compare it to the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'var_smoothing': 1e-09}\n",
      "Best Grid Search Score: 0.9533333333333334\n",
      "Accuracy of Retrained Model: 0.9777777777777777\n",
      "Accuracy of Original Model: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # random_state is used to reproduce the same results\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
    "} # var_smoothing is used to avoid division by zero in the Gaussian Naive Bayes model, it is used to smooth the variance\n",
    "\n",
    "# Create the Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(gnb, param_grid, cv=5, scoring='accuracy') # grids search is used to find the best hyperparameters for the model (param_grid)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and their score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Grid Search Score:\", grid_search.best_score_)\n",
    "\n",
    "# Retrain the model with the best hyperparameters\n",
    "best_gnb = grid_search.best_estimator_\n",
    "best_gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_best_gnb = best_gnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_best_gnb = accuracy_score(y_test, y_pred_best_gnb)\n",
    "print(\"Accuracy of Retrained Model:\", accuracy_best_gnb)\n",
    "\n",
    "# Train the original Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_gnb = accuracy_score(y_test, y_pred_gnb)\n",
    "print(\"Accuracy of Original Model:\", accuracy_gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original and retrained model are the same because the default var_smoothing: 1e-09 is already optimal for the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "#### Problem statement\n",
    "\n",
    "Given the \"obesity_dataset.csv\" containing information of individuals, the task is to train a machine learning model to predict whether an individual is overweight or not and which type of overweight they have.\n",
    "A complete description of the dataset can be found in the paper.\n",
    "\n",
    "Requirements:\n",
    "* Preprocess the data to make it suitable for a machine learning model.\n",
    "* Create a pipeline for you classifier.\n",
    "* Use grid search or random search with 5 fold cross validation to find a good set of parameters for your classifier.\n",
    "* Determine the mean accuracy of the best grid search model by using 5-fold cross validation and a confidence interval of 95%.\n",
    "You can assume the accuracy is normally distributed here without justification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender   Age  Height  Weight family_history_with_overweight FAVC  FCVC  \\\n",
      "0  Female  21.0    1.62    64.0                            yes   no   2.0   \n",
      "1  Female  21.0    1.52    56.0                            yes   no   3.0   \n",
      "2    Male  23.0    1.80    77.0                            yes   no   2.0   \n",
      "3    Male  27.0    1.80    87.0                             no   no   3.0   \n",
      "4    Male  22.0    1.78    89.8                             no   no   2.0   \n",
      "\n",
      "   NCP       CAEC SMOKE  CH2O  SCC  FAF  TUE        CALC  \\\n",
      "0  3.0  Sometimes    no   2.0   no  0.0  1.0          no   \n",
      "1  3.0  Sometimes   yes   3.0  yes  3.0  0.0   Sometimes   \n",
      "2  3.0  Sometimes    no   2.0   no  2.0  1.0  Frequently   \n",
      "3  3.0  Sometimes    no   2.0   no  2.0  0.0  Frequently   \n",
      "4  1.0  Sometimes    no   2.0   no  0.0  0.0   Sometimes   \n",
      "\n",
      "                  MTRANS           NObeyesdad  \n",
      "0  Public_Transportation        Normal_Weight  \n",
      "1  Public_Transportation        Normal_Weight  \n",
      "2  Public_Transportation        Normal_Weight  \n",
      "3                Walking   Overweight_Level_I  \n",
      "4  Public_Transportation  Overweight_Level_II  \n",
      "Gender                            0\n",
      "Age                               0\n",
      "Height                            0\n",
      "Weight                            0\n",
      "family_history_with_overweight    0\n",
      "FAVC                              0\n",
      "FCVC                              0\n",
      "NCP                               0\n",
      "CAEC                              0\n",
      "SMOKE                             0\n",
      "CH2O                              0\n",
      "SCC                               0\n",
      "FAF                               0\n",
      "TUE                               0\n",
      "CALC                              0\n",
      "MTRANS                            0\n",
      "NObeyesdad                        0\n",
      "dtype: int64\n",
      "   Gender   Age  Height  Weight  family_history_with_overweight  FAVC  FCVC  \\\n",
      "0       0  21.0    1.62    64.0                               1     0   2.0   \n",
      "1       0  21.0    1.52    56.0                               1     0   3.0   \n",
      "2       1  23.0    1.80    77.0                               1     0   2.0   \n",
      "3       1  27.0    1.80    87.0                               0     0   3.0   \n",
      "4       1  22.0    1.78    89.8                               0     0   2.0   \n",
      "\n",
      "   NCP  CAEC  SMOKE  CH2O  SCC  FAF  TUE  CALC  MTRANS  NObeyesdad  \n",
      "0  3.0     2      0   2.0    0  0.0  1.0     3       3           1  \n",
      "1  3.0     2      1   3.0    1  3.0  0.0     2       3           1  \n",
      "2  3.0     2      0   2.0    0  2.0  1.0     1       3           1  \n",
      "3  3.0     2      0   2.0    0  2.0  0.0     1       4           5  \n",
      "4  1.0     2      0   2.0    0  0.0  0.0     2       3           6  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Preprocess the data to make it suitable for a machine learning model\n",
    "# Load the dataset\n",
    "df = pd.read_csv('obesity_dataset.csv')\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum()) # No missing values\n",
    "\n",
    "# If missing values:\n",
    "# Fill missing numerical values with the median\n",
    "# df.fillna(df.select_dtypes(include=['number']).median(), inplace=True)\n",
    "# Fill missing categorical values with the mode (most frequent value)\n",
    "# for column in df.select_dtypes(include=['object']).columns:\n",
    "    # df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "\n",
    "# Encode the categorical variables\n",
    "label_encoders = {} # store the label encoders for future use, to decode the data\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder() \n",
    "    df[column] = le.fit_transform(df[column]) # fit_transform is used to encode the data\n",
    "    label_encoders[column] = le\n",
    "# check after encoding\n",
    "print(df.head())\n",
    "\n",
    "# split the data into features and target\n",
    "X = df.drop('NObeyesdad', axis=1) # NObeysdad is the target\n",
    "y = df['NObeyesdad']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a pipeline for you classifier.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a pipeline, chain multiple steps into one, in sequence\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Standardize features: mean=0 and variance=1, normalizes the data, better for certain algorithms that rely on the distribution of the data\n",
    "    ('classifier', RandomForestClassifier(random_state=42))  # Classifier (Random Forest) : it uses multiple decision trees, aggregates the predictions of multiple decision trees\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200], # number of trees in the forest\n",
    "    'classifier__max_depth': [None, 10, 20, 30], # maximum depth of the tree\n",
    "    'classifier__min_samples_split': [2, 5, 10] # minimum number of samples required to split an internal node, node splits only if it has more than min_samples_split samples\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 200}\n",
      "Best Grid Search Score: 0.9375640034508643\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Use grid search or random search with 5 fold cross validation to find a good \n",
    "# set of parameters for your classifier.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and their score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Grid Search Score:\", grid_search.best_score_)\n",
    "\n",
    "# Retrain the model with the best hyperparameters\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.73995272 0.99052133 0.98104265 0.98578199 0.99052133]\n",
      "Mean Accuracy: 0.9375640034508643\n",
      "Standard Deviation: 0.09886813788221695\n",
      "95% Confidence Interval: (np.float64(0.8509038520522678), np.float64(1.0242241548494608))\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Determine the mean accuracy of the best grid search model by using 5-fold \n",
    "# cross validation and a confidence interval of 95%.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "# Perform 5-fold cross-validation on the best model\n",
    "cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Calculate mean accuracy and standard deviation\n",
    "mean_accuracy = np.mean(cv_scores)\n",
    "std_accuracy = np.std(cv_scores)\n",
    "\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n",
    "print(\"Standard Deviation:\", std_accuracy)\n",
    "\n",
    "\n",
    "# Number of samples (folds)\n",
    "n = len(cv_scores)\n",
    "\n",
    "# Calculate the z-value for a 95% confidence interval\n",
    "z_value = norm.ppf(0.975)  # 0.975 because it's two-tailed\n",
    "\n",
    "# Calculate the margin of error\n",
    "margin_of_error = z_value * (std_accuracy / np.sqrt(n))\n",
    "\n",
    "# Calculate the confidence interval\n",
    "confidence_interval = (mean_accuracy - margin_of_error, mean_accuracy + margin_of_error)\n",
    "\n",
    "print(\"95% Confidence Interval:\", confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing:\n",
    "- Handled missing values and encoded categorical variables to make the data suitable for machine learning.\n",
    "\n",
    "Pipeline:\n",
    "- Created a pipeline to standardize features and train a Random Forest classifier.\n",
    "\n",
    "Grid Search:\n",
    "- Used Grid Search with 5-fold cross-validation to find the best hyperparameters.\n",
    "\n",
    "Confidence Interval:\n",
    "- Calculated the mean accuracy and a 95% confidence interval to assess the model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
